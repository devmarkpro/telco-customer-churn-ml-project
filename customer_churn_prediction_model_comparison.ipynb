{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/devmarkpro/churn-prediction-lr-rf-svm-catboost-eda\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction: EDA, Preprocessing, Model Comparison, and Feature Engineering\n",
    "\n",
    "* <a href=\"https://github.com/devmarkpro/telco-customer-churn-ml-project/blob/main/customer_churn_prediction_model_comparison.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub Logo\" style=\"vertical-align: middle; width:16px; height:16px; filter: invert(0.8);\">\n",
    "  GitHub Repo\n",
    "</a>\n",
    "\n",
    "* <a href=\"https://github.com/devmarkpro/telco-customer-churn-ml-project/blob/main/final-report.pdf\" target=\"_blank\">\n",
    "  <img src=\"https://www.freeiconspng.com/uploads/download-16x16-pdf-icon-png-3.png\" alt=\"PDF Report\" style=\"vertical-align: middle; width:16px; height:16px;\">\n",
    "  PDF Report\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview and Problem Statement\n",
    "\n",
    "This project focuses on predicting customer churn in the telecommunications industry. Churn refers to customers who leave a service provider, and predicting churn is crucial for improving customer retention and business profitability.\n",
    "\n",
    "The problem is formulated as a supervised learning task, where the goal is to predict a binary outcome, whether a customer will churn (Yes) or not (No).\n",
    "\n",
    "Since the target variable is categorical with two possible outcomes, this is a binary classification problem. Various classification algorithms will be explored and compared to build a robust predictive model.\n",
    "Through this project, I aim to understand which features most strongly influence customer churn, evaluate model performance using appropriate metrics (e.g., ROC-AUC, F1-score), and identify the best-performing supervised learning approach for this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "\n",
    "The dataset used in this project is the Telco Customer Churn dataset, which contains customer information from a telecommunications company, including demographic details, account data, services subscribed to, and whether the customer has churned. The target variable is Churn, indicating if the customer left (Yes) or stayed (No).\n",
    "\n",
    "The dataset is publicly available on Kaggle: [Telco Customer Churn â€“ Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)\n",
    "\n",
    "It includes 7043 samples and 20 features, with a mix of numerical, binary, and categorical variables.\n",
    "\n",
    "There is also a more extensive version of this dataset published by IBM, which includes more features and additional records:\n",
    "[IBM Telco Customer Churn (extended version)](https://community.ibm.com/community/user/blogs/steven-macko/2019/07/11/telco-customer-churn-1113)\n",
    "\n",
    "However, for the scope of this project, we use the Kaggle version due to its simpler structure, which is more manageable for demonstrating supervised learning techniques within the project constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Explanation\n",
    "\n",
    "The dataset consists of 7043 rows (customers) and 21 columns (20 features + 1 target variable). It is in tabular format, with each row representing a customer and each column representing a demographic, service-related, or account-related attribute.\n",
    "\n",
    "Among the features:\n",
    "\n",
    "- 13 are categorical (e.g., gender, InternetService, Contract)\n",
    "- 3 are numerical (tenure, MonthlyCharges, TotalCharges)\n",
    "- 5 are binary (SeniorCitizen, Partner, Dependents, etc.)\n",
    "\n",
    "The dataset is self-contained (not multi-table) and does not require external data merging. It is relatively small in size (under 1 MB), making it manageable for exploratory data analysis and model experimentation.\n",
    "\n",
    "Here's the full list of columns in the dataset\n",
    "\n",
    "* ``customerID``: Unique ID for each customer\n",
    "* ``gender``: Gender of the customer (`Male` or `Female`)\n",
    "* ``SeniorCitizen``: Indicates if the customer is a senior (`1` = Yes, `0` = No)\n",
    "* ``Partner``: Whether the customer has a partner (`Yes` or `No`)\n",
    "* ``Dependents``: Whether the customer has dependents (`Yes` or `No`)\n",
    "* ``tenure``: Number of months the customer has been with the company\n",
    "* ``PhoneService``: Whether the customer has phone service (`Yes` or `No`)\n",
    "* ``MultipleLines``: Has multiple phone lines (`Yes`, `No`, or `No phone service`)\n",
    "* ``InternetService``: Type of internet service (`DSL`, `Fiber optic`, or `No`)\n",
    "* ``OnlineSecurity``: Whether online security is included (`Yes`, `No`, or `No internet service`)\n",
    "* ``OnlineBackup``: Whether online backup is included (`Yes`, `No`, or `No internet service`)\n",
    "* ``DeviceProtection``: Whether device protection is included (`Yes`, `No`, or `No internet service`)\n",
    "* ``TechSupport``: Whether tech support is included (`Yes`, `No`, or `No internet service`)\n",
    "* ``StreamingTV``: Access to streaming TV (`Yes`, `No`, or `No internet service`)\n",
    "* ``StreamingMovies``: Access to streaming movies (`Yes`, `No`, or `No internet service`)\n",
    "* ``Contract``: Type of contract (`Month-to-month`, `One year`, `Two year`)\n",
    "* ``PaperlessBilling``: Whether billing is paperless (`Yes` or `No`)\n",
    "* ``PaymentMethod``: Method of payment (`Electronic check`, `Mailed check`, etc.)\n",
    "* ``MonthlyCharges``: Monthly amount charged to the customer\n",
    "* ``TotalCharges``: Total amount charged over the tenure\n",
    "* ``Churn``: Target variable. Whether the customer churned (`Yes` or `No`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.021242Z",
     "start_time": "2025-06-02T17:13:35.01906Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:08:06.200393Z",
     "iopub.status.busy": "2025-06-04T19:08:06.199652Z",
     "iopub.status.idle": "2025-06-04T19:08:06.216652Z",
     "shell.execute_reply": "2025-06-04T19:08:06.215256Z",
     "shell.execute_reply.started": "2025-06-04T19:08:06.200355Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in local\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# This script detects the environment in which it is running (Colab, Kaggle, or local), it will be helpfull for\n",
    "# setting up paths or configurations that differ based on the environment.\n",
    "\n",
    "def detect_environment():\n",
    "    if 'google.colab' in sys.modules:\n",
    "        return 'colab'\n",
    "    elif 'kaggle' in sys.modules or os.path.exists('/kaggle'):\n",
    "        return 'kaggle'\n",
    "    else:\n",
    "        return 'local'\n",
    "env = detect_environment()\n",
    "print(f\"running in {env}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:08:29.436867Z",
     "iopub.status.busy": "2025-06-04T19:08:29.436533Z",
     "iopub.status.idle": "2025-06-04T19:09:07.776083Z",
     "shell.execute_reply": "2025-06-04T19:09:07.774611Z",
     "shell.execute_reply.started": "2025-06-04T19:08:29.436835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# dependencies in local env is managed in conda environment, so we don't need to install them here\n",
    "if env != 'local':\n",
    "    !pip install -U scikit-learn==1.3.2 imbalanced-learn==0.11.0 --quiet\n",
    "    !pip install seaborn==0.13.2 --quiet\n",
    "    !pip install matplotlib==3.10.0 --quiet\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*figure layout has changed to tight.*\")\n",
    "if env == 'kaggle':\n",
    "    import kagglehub\n",
    "    \n",
    "def get_dataset_path():\n",
    "    path = ''\n",
    "    \n",
    "    if env == \"kaggle\":\n",
    "        path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "        path += '/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "    if env == 'local':\n",
    "        path = \"./data/Telco-Customer-Churn.csv\"\n",
    "    if env == 'colab':\n",
    "        raise ValueError(\"colab env is not supported yet\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_result_path():\n",
    "    path = ''\n",
    "    \n",
    "    if env == \"kaggle\":\n",
    "        path = '/kaggle/working'\n",
    "    if env == 'local':\n",
    "        path = \"./results\"\n",
    "    if env == 'colab':\n",
    "        raise ValueError(\"colab env is not supported yet\")\n",
    "    return path    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.003471Z",
     "start_time": "2025-06-02T17:13:34.281852Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:07.77804Z",
     "iopub.status.busy": "2025-06-04T19:09:07.777705Z",
     "iopub.status.idle": "2025-06-04T19:09:09.262846Z",
     "shell.execute_reply": "2025-06-04T19:09:09.261772Z",
     "shell.execute_reply.started": "2025-06-04T19:09:07.778009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from catboost import cv, Pool, CatBoostClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.093833Z",
     "start_time": "2025-06-02T17:13:35.091512Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:21.108051Z",
     "iopub.status.busy": "2025-06-04T19:09:21.106735Z",
     "iopub.status.idle": "2025-06-04T19:09:21.27097Z",
     "shell.execute_reply": "2025-06-04T19:09:21.269765Z",
     "shell.execute_reply.started": "2025-06-04T19:09:21.108015Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: ./data/Telco-Customer-Churn.csv\n",
      "Result path: ./results\n"
     ]
    }
   ],
   "source": [
    "# Constants for the EDA, preprocessing, and modeling\n",
    "TARGET_COLUMN = \"Churn\"\n",
    "RANDOM_STATE = 42\n",
    "K_FOLDS = 5\n",
    "RESULT_PATH = get_result_path()\n",
    "RESULT_FINE_NAME = \"model_comparison_results\"\n",
    "RESULT_FINE_EXT = \"csv\"\n",
    "DATASET_PATH = get_dataset_path()\n",
    "TEST_SIZE = 0.2\n",
    "SCORING = 'roc_auc'\n",
    "\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Result path: {RESULT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.151005Z",
     "start_time": "2025-06-02T17:13:35.122578Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:22.442928Z",
     "iopub.status.busy": "2025-06-04T19:09:22.441681Z",
     "iopub.status.idle": "2025-06-04T19:09:22.496102Z",
     "shell.execute_reply": "2025-06-04T19:09:22.494729Z",
     "shell.execute_reply.started": "2025-06-04T19:09:22.442879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.184493Z",
     "start_time": "2025-06-02T17:13:35.180373Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:23.570977Z",
     "iopub.status.busy": "2025-06-04T19:09:23.569668Z",
     "iopub.status.idle": "2025-06-04T19:09:23.577255Z",
     "shell.execute_reply": "2025-06-04T19:09:23.575914Z",
     "shell.execute_reply.started": "2025-06-04T19:09:23.570931Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (7043, 21)\n",
      "columns: ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n"
     ]
    }
   ],
   "source": [
    "print(f'shape: {df.shape}')\n",
    "print(f'columns: {df.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.221404Z",
     "start_time": "2025-06-02T17:13:35.215165Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:24.076162Z",
     "iopub.status.busy": "2025-06-04T19:09:24.074802Z",
     "iopub.status.idle": "2025-06-04T19:09:24.085723Z",
     "shell.execute_reply": "2025-06-04T19:09:24.083918Z",
     "shell.execute_reply.started": "2025-06-04T19:09:24.076125Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SeniorCitizen is binary coloumn and it already `int64` type, however, we need to covert columns like gender, Partner, Dependents, etc to `category` type.\n",
    "\n",
    "In general, we have three categories of columns:\n",
    "1. Numerical columns\n",
    "2. Categorical columns\n",
    "3. Binary columns\n",
    "\n",
    "However, I also consider separating features based on their domain, such as service, demographic, and payment features. This will help in understanding the data better later on.\n",
    "\n",
    "One important thing to note is that the `TotalCharges` column is a numerical column, but it has a type of `object` due to some non-numeric values. We need to convert it to a numeric type and handle any errors that arise from non-numeric values.\n",
    "\n",
    "Before we proceed with the conversion, let's check the unique values in each column to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.28387Z",
     "start_time": "2025-06-02T17:13:35.270226Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:26.148965Z",
     "iopub.status.busy": "2025-06-04T19:09:26.148536Z",
     "iopub.status.idle": "2025-06-04T19:09:26.182217Z",
     "shell.execute_reply": "2025-06-04T19:09:26.181117Z",
     "shell.execute_reply.started": "2025-06-04T19:09:26.148936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customerID: 7043 unique values : ['7590-VHVEG' '5575-GNVDE' '3668-QPYBK' ... '4801-JZAZL' '8361-LTMKD'\n",
      " '3186-AJIEK'] unique values\n",
      "gender: 2 unique values : ['Female' 'Male'] unique values\n",
      "SeniorCitizen: 2 unique values : [0 1] unique values\n",
      "Partner: 2 unique values : ['Yes' 'No'] unique values\n",
      "Dependents: 2 unique values : ['No' 'Yes'] unique values\n",
      "tenure: 73 unique values : [ 1 34  2 45  8 22 10 28 62 13 16 58 49 25 69 52 71 21 12 30 47 72 17 27\n",
      "  5 46 11 70 63 43 15 60 18 66  9  3 31 50 64 56  7 42 35 48 29 65 38 68\n",
      " 32 55 37 36 41  6  4 33 67 23 57 61 14 20 53 40 59 24 44 19 54 51 26  0\n",
      " 39] unique values\n",
      "PhoneService: 2 unique values : ['No' 'Yes'] unique values\n",
      "MultipleLines: 3 unique values : ['No phone service' 'No' 'Yes'] unique values\n",
      "InternetService: 3 unique values : ['DSL' 'Fiber optic' 'No'] unique values\n",
      "OnlineSecurity: 3 unique values : ['No' 'Yes' 'No internet service'] unique values\n",
      "OnlineBackup: 3 unique values : ['Yes' 'No' 'No internet service'] unique values\n",
      "DeviceProtection: 3 unique values : ['No' 'Yes' 'No internet service'] unique values\n",
      "TechSupport: 3 unique values : ['No' 'Yes' 'No internet service'] unique values\n",
      "StreamingTV: 3 unique values : ['No' 'Yes' 'No internet service'] unique values\n",
      "StreamingMovies: 3 unique values : ['No' 'Yes' 'No internet service'] unique values\n",
      "Contract: 3 unique values : ['Month-to-month' 'One year' 'Two year'] unique values\n",
      "PaperlessBilling: 2 unique values : ['Yes' 'No'] unique values\n",
      "PaymentMethod: 4 unique values : ['Electronic check' 'Mailed check' 'Bank transfer (automatic)'\n",
      " 'Credit card (automatic)'] unique values\n",
      "MonthlyCharges: 1585 unique values : [29.85 56.95 53.85 ... 63.1  44.2  78.7 ] unique values\n",
      "TotalCharges: 6531 unique values : ['29.85' '1889.5' '108.15' ... '346.45' '306.6' '6844.5'] unique values\n",
      "Churn: 2 unique values : ['No' 'Yes'] unique values\n"
     ]
    }
   ],
   "source": [
    "# unique values in each column\n",
    "for col in df.columns:\n",
    "    print(f'{col}: {df[col].nunique()} unique values : {df[col].unique()} unique values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `gender`, `Partner`, `Dependents`, `PhoneService`, and `PaperlessBilling` columns have binary values, while the `InternetService`, `MultipleLines`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, and `StreamingMovies` columns are categorical with multiple unique values. The `Contract` and `PaymentMethod` columns also have categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.411043Z",
     "start_time": "2025-06-02T17:13:35.408764Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:28.446673Z",
     "iopub.status.busy": "2025-06-04T19:09:28.446311Z",
     "iopub.status.idle": "2025-06-04T19:09:28.453939Z",
     "shell.execute_reply": "2025-06-04T19:09:28.452616Z",
     "shell.execute_reply.started": "2025-06-04T19:09:28.446646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# During the EDA, we will use features based on the following categories:\n",
    "# 1. Demographic\n",
    "# 2. Service\n",
    "# 3. Payment\n",
    "\n",
    "demographic_features = [\"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\"]\n",
    "service_features = [\"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\",\n",
    "                    \"OnlineBackup\", \"DeviceProtection\", \"StreamingTV\", \"StreamingMovies\", \"TechSupport\"]\n",
    "payment_features = [\"Contract\", \"PaperlessBilling\", \"PaymentMethod\"]\n",
    "\n",
    "binary_features = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'SeniorCitizen']\n",
    "\n",
    "# Categorical features are those that are not binary and are not numeric\n",
    "categorical_features = list(set(service_features + payment_features) - set(binary_features))\n",
    "\n",
    "# Numeric features are those that are not categorical and not binary, we need to convert them to numeric if they are not already\n",
    "numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "features = demographic_features + service_features + payment_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.477951Z",
     "start_time": "2025-06-02T17:13:35.473013Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:29.601178Z",
     "iopub.status.busy": "2025-06-04T19:09:29.600777Z",
     "iopub.status.idle": "2025-06-04T19:09:29.613055Z",
     "shell.execute_reply": "2025-06-04T19:09:29.611924Z",
     "shell.execute_reply.started": "2025-06-04T19:09:29.601149Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalCharges type before conversion: object, total null values: 0\n"
     ]
    }
   ],
   "source": [
    "# handling TotalCharges column\n",
    "print(f'TotalCharges type before conversion: {df[\"TotalCharges\"].dtype}, total null values: {df[\"TotalCharges\"].isna().sum()}')\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While converting the `TotalCharges` column to numeric, we use `errors='coerce'` to convert any non-numeric values to NaN. This is important because it allows us to handle any invalid entries without causing the entire conversion to fail. After this conversion, we can decide how to handle these NaN values, either by dropping them or filling them with a specific value.\n",
    "Let's check how many NaN values we have in the `TotalCharges` column and then drop them if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.572673Z",
     "start_time": "2025-06-02T17:13:35.566403Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:33.115005Z",
     "iopub.status.busy": "2025-06-04T19:09:33.113653Z",
     "iopub.status.idle": "2025-06-04T19:09:33.121187Z",
     "shell.execute_reply": "2025-06-04T19:09:33.1201Z",
     "shell.execute_reply.started": "2025-06-04T19:09:33.114964Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalCharges type after conversion: float64, total null values: 11\n"
     ]
    }
   ],
   "source": [
    "print(f'TotalCharges type after conversion: {df[\"TotalCharges\"].dtype}, total null values: {df[\"TotalCharges\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data type of `TotalCharges` is `float64`, but we have 11 rows with NaN values after the conversion. Let's check these rows in the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.642886Z",
     "start_time": "2025-06-02T17:13:35.623261Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:34.478203Z",
     "iopub.status.busy": "2025-06-04T19:09:34.477857Z",
     "iopub.status.idle": "2025-06-04T19:09:34.523623Z",
     "shell.execute_reply": "2025-06-04T19:09:34.522662Z",
     "shell.execute_reply.started": "2025-06-04T19:09:34.478178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "temp_df = pd.read_csv(DATASET_PATH, delimiter=',')\n",
    "temp_df.loc[df[df['TotalCharges'].isnull()].index, 'TotalCharges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that these rows have no values in the `TotalCharges` column, which is why they were converted to NaN. Since these rows do not provide any useful information for our analysis and the total number of rows is small (11), we can safely drop them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.702431Z",
     "start_time": "2025-06-02T17:13:35.696459Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:36.363589Z",
     "iopub.status.busy": "2025-06-04T19:09:36.362659Z",
     "iopub.status.idle": "2025-06-04T19:09:36.377599Z",
     "shell.execute_reply": "2025-06-04T19:09:36.376337Z",
     "shell.execute_reply.started": "2025-06-04T19:09:36.363542Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total null values: 0\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['TotalCharges'])\n",
    "print(f'Total null values: {df[\"TotalCharges\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the previous steps, `customerID` is a unique identifier for each customer and does not provide any useful information for our analysis. Therefore, we can drop this column from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.814074Z",
     "start_time": "2025-06-02T17:13:35.807474Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:37.91226Z",
     "iopub.status.busy": "2025-06-04T19:09:37.911943Z",
     "iopub.status.idle": "2025-06-04T19:09:37.920911Z",
     "shell.execute_reply": "2025-06-04T19:09:37.919671Z",
     "shell.execute_reply.started": "2025-06-04T19:09:37.912238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# drop customerID column\n",
    "df = df.drop(columns=['customerID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:35.865819Z",
     "start_time": "2025-06-02T17:13:35.849564Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:38.315461Z",
     "iopub.status.busy": "2025-06-04T19:09:38.315087Z",
     "iopub.status.idle": "2025-06-04T19:09:38.341314Z",
     "shell.execute_reply": "2025-06-04T19:09:38.340259Z",
     "shell.execute_reply.started": "2025-06-04T19:09:38.315434Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total null values in the dataset: 0\n",
      "Total duplicated rows in the dataset: 22\n"
     ]
    }
   ],
   "source": [
    "# check null and duplicated values in the dataset\n",
    "print(f'Total null values in the dataset: {df.isna().sum().sum()}')\n",
    "print(f'Total duplicated rows in the dataset: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 22 duplicated rows in the dataset, however, considering these are belong to different customers, we can keep them as they are. We also have no null values in the dataset, which is great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:36.181552Z",
     "start_time": "2025-06-02T17:13:35.957471Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:40.270933Z",
     "iopub.status.busy": "2025-06-04T19:09:40.270484Z",
     "iopub.status.idle": "2025-06-04T19:09:40.621676Z",
     "shell.execute_reply": "2025-06-04T19:09:40.62042Z",
     "shell.execute_reply.started": "2025-06-04T19:09:40.270901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Countplot\n",
    "sns.countplot(x=TARGET_COLUMN, data=df, ax=axes[0])\n",
    "axes[0].set_title(\"Distribution of Customer Churn\")\n",
    "axes[0].set_xlabel(TARGET_COLUMN)\n",
    "axes[0].set_ylabel(\"Number of Customers\")\n",
    "axes[0].bar_label(axes[0].containers[0])\n",
    "\n",
    "# Right: Pie Chart\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "churn_percent = churn_counts / churn_counts.sum() * 100\n",
    "axes[1].pie(churn_percent, labels=churn_percent.index, autopct='%1.1f%%',\n",
    "            startangle=90, colors=['#66c2a5', '#fc8d62'])\n",
    "axes[1].set_title(\"Churn Distribution (Percentage)\")\n",
    "axes[1].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distribution of the target variable `Churn`, we can see that the dataset is imbalanced, with a higher number of customers who did not churn compared to those who did. This is a common scenario in churn prediction tasks and will require special attention during model training to ensure that the model does not become biased towards the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the numerical feature analysis. We will check the distribution of each numerical feature and their correlation with the target variable `Churn`. Let's start with a boxplot to visualize the distribution of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:36.406575Z",
     "start_time": "2025-06-02T17:13:36.209789Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:44.231939Z",
     "iopub.status.busy": "2025-06-04T19:09:44.231539Z",
     "iopub.status.idle": "2025-06-04T19:09:44.705297Z",
     "shell.execute_reply": "2025-06-04T19:09:44.704441Z",
     "shell.execute_reply.started": "2025-06-04T19:09:44.231911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i in range(0, len(numeric_features)):\n",
    "    plt.subplot(1, len(numeric_features), i + 1)\n",
    "    sns.boxplot(data=df, y=numeric_features[i])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of Numerical Features**\n",
    "\n",
    "The boxplots for `tenure`, `MonthlyCharges`, and `TotalCharges` provide insight into the distribution and spread of these numerical variables. The `tenure` feature shows a relatively wide distribution, with a median around 29 months and no visible outliers, indicating a stable range of customer contract lengths. Similarly, `MonthlyCharges` is moderately right-skewed, with most customers paying between 35 and 90 per month. There are no extreme values beyond the whiskers, suggesting that premium-tier customers fall within a reasonable range.\n",
    "\n",
    "The `TotalCharges` feature exhibits a pronounced right skew, which is expected given its dependency on both tenure and monthly charges. Although some customers have paid significantly more over time, these values are within the expected bounds and are not treated as outliers. Overall, the numerical features appear well-behaved and do not require special treatment for outliers at this stage of the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's analyze the correlation of these numerical features with the target variable `Churn`. We will use a boxplot to visualize the distribution of each numerical feature based on the `Churn` status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:36.653643Z",
     "start_time": "2025-06-02T17:13:36.420796Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:52.94738Z",
     "iopub.status.busy": "2025-06-04T19:09:52.947061Z",
     "iopub.status.idle": "2025-06-04T19:09:53.635479Z",
     "shell.execute_reply": "2025-06-04T19:09:53.634312Z",
     "shell.execute_reply.started": "2025-06-04T19:09:52.947344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, len(numeric_features), figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Loop over numeric features\n",
    "for i, col in enumerate(numeric_features):\n",
    "    sns.boxplot(data=df, y=col, x='Churn', ax=axes[i], width=0.5, notch=True)\n",
    "    axes[i].set_title(f\"{col}\", fontsize=13)\n",
    "    axes[i].set_xlabel(TARGET_COLUMN, fontsize=11)\n",
    "    axes[i].set_ylabel(col, fontsize=11)\n",
    "    axes[i].tick_params(labelsize=10)\n",
    "\n",
    "# Add main title\n",
    "fig.suptitle(\"Boxplots of Numerical Features by Churn\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplots clearly show different patterns in the numerical features based on churn status. Customers who did not churn have a much higher median `tenure` compared to those who churned, meaning loyal customers tend to stay longer. In contrast, customers who churned often have very short tenure, suggesting that many leave early in their contract period.\n",
    "\n",
    "For `MonthlyCharges`, customers who churn generally pay more on average than those who stay. This could suggest that higher monthly costs may lead to dissatisfaction or affordability issues. The `TotalCharges` feature also shows a strong difference. Since it is the product of tenure and monthly charges, it makes sense that customers who churn have much lower total charges. This supports the idea that they often leave early. Overall, these numerical features provide useful signals for predicting churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the numerical features, Let's also look at the histogram with KDE (Kernel Density Estimate) for each numerical feature. This will help us visualize the distribution of values and identify any skewness or unusual patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:37.072068Z",
     "start_time": "2025-06-02T17:13:36.666004Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:54.052744Z",
     "iopub.status.busy": "2025-06-04T19:09:54.052378Z",
     "iopub.status.idle": "2025-06-04T19:09:55.094869Z",
     "shell.execute_reply": "2025-06-04T19:09:55.093718Z",
     "shell.execute_reply.started": "2025-06-04T19:09:54.052718Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, len(numeric_features), figsize=(20, 6), constrained_layout=True)\n",
    "\n",
    "for i, col in enumerate(numeric_features):\n",
    "    sns.histplot(df[col], kde=True, ax=axes[i], color=\"skyblue\", edgecolor=\"white\", linewidth=1.3)\n",
    "    axes[i].set_title(f\"Distribution of {col}\", fontsize=13)\n",
    "    axes[i].set_xlabel(col, fontsize=11)\n",
    "    axes[i].set_ylabel(\"Count\", fontsize=11)\n",
    "    axes[i].tick_params(labelsize=10)\n",
    "\n",
    "fig.suptitle(\"Histograms with KDE of Numerical Features\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms with KDE curves provide a deeper view of how the numerical features are distributed. The `tenure` variable shows a bimodal pattern, with many customers at the beginning (around 0â€“10 months) and another peak around 70 months. This suggests that some customers leave very early, while others tend to stay for many years.\n",
    "\n",
    "The `MonthlyCharges` feature appears roughly right-skewed, with a concentration of customers around 70â€“90. However, there is also a significant number of customers paying less than 30. The `TotalCharges` variable is strongly right-skewed, which makes sense since it accumulates over time based on both tenure and monthly charges. Many customers have relatively low total charges, likely due to short tenure. These distributions confirm that feature scaling or transformations might be useful before modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, we use `sns.pairplot` to visualize the relationships between numerical features and the target variable `Churn`. This will help us see how these features interact with each other and with churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:39.095521Z",
     "start_time": "2025-06-02T17:13:37.084818Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:09:58.051139Z",
     "iopub.status.busy": "2025-06-04T19:09:58.05069Z",
     "iopub.status.idle": "2025-06-04T19:10:04.04287Z",
     "shell.execute_reply": "2025-06-04T19:10:04.041833Z",
     "shell.execute_reply.started": "2025-06-04T19:09:58.051111Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sns.pairplot(df[['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']], hue='Churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairplot shows the relationships between `tenure`, `MonthlyCharges`, and `TotalCharges`, colored by churn status. We can observe that customers who churned (orange) tend to have lower `tenure` and lower `TotalCharges`, but often have higher `MonthlyCharges`. This supports earlier findings that customers who leave usually do so early and may be paying more per month.\n",
    "\n",
    "There is also a strong positive linear relationship between `tenure` and `TotalCharges`, which is expected since longer-tenure customers accumulate higher total charges. The density plots along the diagonal show clear separation between churned and non-churned customers, especially in `tenure` and `TotalCharges`. Overall, this plot confirms that these numerical features are useful for distinguishing between churned and retained customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's analyze the correlation matrix for the numerical features, including the target variable `Churn`. This will help us quantify the relationships between these features and identify any strong correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:39.166663Z",
     "start_time": "2025-06-02T17:13:39.104325Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:10:07.150193Z",
     "iopub.status.busy": "2025-06-04T19:10:07.149761Z",
     "iopub.status.idle": "2025-06-04T19:10:07.488001Z",
     "shell.execute_reply": "2025-06-04T19:10:07.486016Z",
     "shell.execute_reply.started": "2025-06-04T19:10:07.150167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "correlation_matrix = df[numeric_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation matrix focuses only on continuous numerical features: `tenure`, `MonthlyCharges`, and `TotalCharges`. The strongest correlation is between `tenure` and `TotalCharges` (0.83), which is expected since the total charges accumulate over time. There is also a moderate positive correlation between `MonthlyCharges` and `TotalCharges` (0.65), showing that customers with higher monthly fees tend to have higher total charges.\n",
    "\n",
    "The correlation between `tenure` and `MonthlyCharges` is relatively weak (0.25), suggesting that the monthly rate does not strongly depend on how long a customer stays. Overall, this heatmap confirms that `TotalCharges` is influenced by both other features, and that `tenure` and `MonthlyCharges` each contribute useful, but different, information to churn prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIF Analysis\n",
    "\n",
    "To get a better insight into the correlation of features, let's take a look at the VIF (Variance inflaction factor) of the features. VIP helps us to identify multicollinearity among features. A high VIF indicates that a feature is highly correlated with other features, which can lead to issue in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:10:10.853277Z",
     "iopub.status.busy": "2025-06-04T19:10:10.852943Z",
     "iopub.status.idle": "2025-06-04T19:10:10.894358Z",
     "shell.execute_reply": "2025-06-04T19:10:10.89315Z",
     "shell.execute_reply.started": "2025-06-04T19:10:10.853253Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature       VIF\n",
      "0          tenure  5.844646\n",
      "1  MonthlyCharges  3.225293\n",
      "2    TotalCharges  9.526697\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Convert target to binary\n",
    "df[TARGET_COLUMN] = df[TARGET_COLUMN].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "# Define target and numeric features (exclude TotalSpendEstimate)\n",
    "numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']  # No TotalSpendEstimate\n",
    "\n",
    "# Prepare X matrix\n",
    "X = df[numeric_features].copy()\n",
    "X['Intercept'] = 1  # Add intercept for VIF computation\n",
    "\n",
    "# Compute VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Drop intercept from results (optional)\n",
    "vif_data = vif_data[vif_data['Feature'] != 'Intercept']\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further investigate multicollinearity, we calculated the Variance Inflation Factor (VIF) for the three numerical features. The results show that `TotalCharges` has a high VIF value of 9.53, indicating strong multicollinearity with other features. This makes sense given that `TotalCharges` is closely related to both `tenure` and `MonthlyCharges`.\n",
    "\n",
    "In contrast, `tenure` and `MonthlyCharges` have lower VIF values of 5.84 and 3.23, which are within acceptable ranges. These findings suggest that if we use linear models like logistic regression, it may be beneficial to drop `TotalCharges` to avoid instability caused by multicollinearity. However, for tree-based models like Random Forest, all features can be safely included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Numerical Feature Significance\n",
    "\n",
    "Based on the exploratory analysis, including boxplots, distribution plots, pairplots, and correlation matrix, we can draw conclusions about the significance of the numerical features in relation to the target variable `Churn`.\n",
    "\n",
    "The feature `tenure` appears to be the most significant indicator of churn. Customers who churn tend to have much shorter tenure, suggesting they leave early in their customer lifecycle. This is clearly visible in both the boxplots and pairplots, where churned customers are concentrated at low tenure values.\n",
    "\n",
    "`MonthlyCharges` also shows a moderate association with churn. Customers who churn are more likely to be paying higher monthly fees, which might indicate dissatisfaction or affordability issues. Although the relationship is weaker than with tenure, it still adds meaningful information.\n",
    "\n",
    "On the other hand, `TotalCharges` is highly correlated with `tenure` (correlation coefficient â‰ˆ 0.83) and is largely derived from it. While it does reflect overall customer spending, it may not add much independent value for churn prediction and could introduce multicollinearity if used alongside `tenure`.\n",
    "\n",
    "To verify this, we computed the Variance Inflation Factor (VIF) for each feature. The VIF score for `TotalCharges` was 9.53â€”close to the commonly used threshold of 10â€”indicating a high degree of multicollinearity. In contrast, `tenure` and `MonthlyCharges` had acceptable VIF values of 5.84 and 3.23, respectively. This supports the conclusion that `tenure` and `MonthlyCharges` should be prioritized, and `TotalCharges` should be excluded from linear models or used cautiously depending on the modeling technique.\n",
    "\n",
    "In conclusion, `tenure` and `MonthlyCharges` are significant numerical features for predicting churn, while `TotalCharges` shows redundancy and multicollinearity risk that may affect certain models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical features, we first plot the distribution of each feature based on their values (categories) and then analyze their correlation with the target variable `Churn`. We will use count plots for categorical features and a bar plot for the correlation analysis. Let's start with the count plots for each categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:40.016301Z",
     "start_time": "2025-06-02T17:13:39.182906Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:10:15.254143Z",
     "iopub.status.busy": "2025-06-04T19:10:15.252725Z",
     "iopub.status.idle": "2025-06-04T19:10:19.069965Z",
     "shell.execute_reply": "2025-06-04T19:10:19.068414Z",
     "shell.execute_reply.started": "2025-06-04T19:10:15.254097Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# plotting the distribution of categorical features in a 3 column layout\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = (len(features) + n_cols - 1) // n_cols\n",
    "feature_categories = (\n",
    "    ['Demographic'] * len(demographic_features) +\n",
    "    ['Service'] * len(service_features) +\n",
    "    ['Payment'] * len(payment_features)\n",
    ")\n",
    "category_colors = {\n",
    "    'Demographic': sns.color_palette(\"Set2\")[0],\n",
    "    'Service': sns.color_palette(\"Set2\")[1],\n",
    "    'Payment': sns.color_palette(\"Set2\")[2]\n",
    "}\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(18, n_rows * 3.5), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    ax = axes[i]\n",
    "    category = feature_categories[i]\n",
    "    sns.countplot(x=df[feature], ax=ax, color=category_colors[category])\n",
    "    ax.bar_label(ax.containers[0], fontsize=9)\n",
    "    ax.set_title(f\"{feature_categories[i]}: {feature}\", fontsize=12)\n",
    "    ax.set_ylabel(\"Customers\", fontsize=10)\n",
    "    ax.set_xlabel(\"\", fontsize=10)\n",
    "    ax.tick_params(axis='x', labelrotation=20, labelsize=9)\n",
    "    ax.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "for j in range(len(features), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "fig.suptitle(\"Customer Distribution by Feature Category\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count plots show how the values of each categorical feature are distributed. The demographic features are quite balanced. For example, the number of male and female customers is similar. Most customers are not senior citizens and do not have dependents.\n",
    "\n",
    "For the service-related features, we see that many customers have phone service and internet. However, fewer people use services like tech support, device protection, or online security. These services might affect whether a customer stays or leaves. In the payment features, most customers have month-to-month contracts and use electronic checks. These choices may be related to a higher risk of churn because short contracts and certain payment types often mean lower customer loyalty.\n",
    "\n",
    "These plots help us understand the data better and will guide the analysis of how these features relate to churn in the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyze the correlation of these categorical features with the target variable `Churn`. We will use a bar plot to visualize the correlation coefficients for each feature with respect to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:13:40.640459Z",
     "start_time": "2025-06-02T17:13:40.033635Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T19:10:23.77503Z",
     "iopub.status.busy": "2025-06-04T19:10:23.774376Z",
     "iopub.status.idle": "2025-06-04T19:10:26.226719Z",
     "shell.execute_reply": "2025-06-04T19:10:26.225546Z",
     "shell.execute_reply.started": "2025-06-04T19:10:23.774996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "n_cols = 3 # Number of columns for the plots\n",
    "n_rows = (len(categorical_features) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_features):\n",
    "    sns.countplot(x=col, hue='Churn', data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col} by Churn')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for j in range(len(categorical_features), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the grouped count plots, we can observe how different categories relate to the target variable `Churn`. Some features show a clear difference between customers who churned and those who did not.\n",
    "\n",
    "For example, customers on **month-to-month contracts** show a much higher churn rate compared to those with one- or two-year contracts. This suggests that customers with flexible plans are more likely to leave. Similarly, people who use **electronic check** as their payment method also have higher churn, while customers who use credit cards or bank transfers tend to stay longer.\n",
    "\n",
    "Service-related features like **OnlineSecurity**, **TechSupport**, and **DeviceProtection** also show strong patterns: churn is higher among those who do not use these services. This may indicate that customers who feel less supported or protected are more likely to leave. In contrast, for features like **StreamingTV** or **StreamingMovies**, the churn rates are more balanced across categories, meaning they might not be strong predictors of churn.\n",
    "\n",
    "Overall, contract type, payment method, and use of support-related services appear to have the strongest correlation with churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed the EDA and identified the features that are significant for predicting churn. Now, we need to preprocess the data to prepare it for modeling. Some of the analysis are only possible after preprocessing, such as encoding categorical variables and scaling numerical features. So let's proceed with the preprocessing steps and then re-evaluate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset has binary, categorical, and numerical features. We will handle each type appropriately:\n",
    "1. **Binary Features**: Convert binary features to numerical values (0 and 1).\n",
    "2. **Categorical Features**: Use one-hot encoding for categorical features with more than two categories.\n",
    "3. **Numerical Features**: Scale numerical features to have zero mean and unit variance.\n",
    "We will use `pd.get_dummies` for one-hot encoding categorical features and `StandardScaler` for scaling numerical features. Let's implement these preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a reusable code, we will create `load_and_preprocess` function that loads the dataset and preprocess it based on the parameters passed to it. This is necessary because we will also use some model that doesn't required preprocessing, such as `CatBoostClassifier`, which can handle categorical features directly. \n",
    "\n",
    "Here are the steps we will take in the `load_and_preprocess` function:\n",
    "1. Load the dataset from the give path.\n",
    "2. drop the `customerID` column as it has no predictive value.\n",
    "3. Convert `TotalCharges` to numeric and drop rows with `NaN` values.\n",
    "4. Encode binary features to numerical values (0 and 1)\n",
    "5. encode target variable `Churn` to numerical values (0 and 1).\n",
    "6. Map gender to 0 and 1.\n",
    "7. one-hot encode categorical features with more than two categories.\n",
    "8. Scale numerical features using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:10:32.529398Z",
     "iopub.status.busy": "2025-06-04T19:10:32.529014Z",
     "iopub.status.idle": "2025-06-04T19:10:32.539105Z",
     "shell.execute_reply": "2025-06-04T19:10:32.537853Z",
     "shell.execute_reply.started": "2025-06-04T19:10:32.529371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(\n",
    "    filepath: str,\n",
    "    drop_aux=False,\n",
    "    encode_binary=False,\n",
    "    map_gender=False,\n",
    "    one_hot_encoding=False,\n",
    "    scale_numeric=False,\n",
    "    to_numeric=False,\n",
    "    encode_target=True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and preprocess the Telco Customer Churn dataset.\n",
    "    Parameters:\n",
    "    - filepath: str, path to the dataset CSV file.\n",
    "    - drop_aux: bool, whether to drop auxiliary columns like customerID.\n",
    "    - encode_binary: bool, whether to encode binary features (Yes/No) as 1/0.\n",
    "    - map_gender: bool, whether to map gender to binary\n",
    "    - one_hot_encoding: bool, whether to apply one-hot encoding to categorical features.\n",
    "    - scale_numeric: bool, whether to scale numeric features using StandardScaler.\n",
    "    - to_numeric: bool, whether to convert TotalCharges to numeric and drop missing values.\n",
    "    - encode_target: bool, whether to encode the target variable (Churn) as 1/0.\n",
    "    \"\"\"\n",
    "    churn_df = pd.read_csv(filepath)\n",
    "    if drop_aux:\n",
    "        # Drop customerID\n",
    "        churn_df = churn_df.drop(columns=[\"customerID\"])\n",
    "\n",
    "    if to_numeric:\n",
    "        # Convert TotalCharges to numeric and drop missing values\n",
    "        churn_df[\"TotalCharges\"] = pd.to_numeric(\n",
    "            churn_df[\"TotalCharges\"], errors=\"coerce\"\n",
    "        )\n",
    "        churn_df = churn_df.dropna(subset=[\"TotalCharges\"])\n",
    "\n",
    "    if encode_binary:\n",
    "        # Encode binary features\n",
    "        binary_cols = [\"Partner\", \"Dependents\", \"PhoneService\", \"PaperlessBilling\"]\n",
    "        for col in binary_cols:\n",
    "            churn_df[col] = churn_df[col].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "    if encode_target:\n",
    "        # Encode target variable\n",
    "        churn_df[TARGET_COLUMN] = churn_df[TARGET_COLUMN].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "    if map_gender:\n",
    "        # Map gender\n",
    "        churn_df[\"gender\"] = churn_df[\"gender\"].map({\"Male\": 1, \"Female\": 0})\n",
    "\n",
    "    if one_hot_encoding:\n",
    "        # One-hot encode remaining categorical variables\n",
    "        categorical_cols = churn_df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        churn_df = pd.get_dummies(churn_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    if scale_numeric:\n",
    "        # Scale numeric features\n",
    "        numeric_cols = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "        scaler = StandardScaler()\n",
    "        churn_df[numeric_cols] = scaler.fit_transform(churn_df[numeric_cols])\n",
    "\n",
    "    return churn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:10:34.621213Z",
     "iopub.status.busy": "2025-06-04T19:10:34.620831Z",
     "iopub.status.idle": "2025-06-04T19:10:34.691333Z",
     "shell.execute_reply": "2025-06-04T19:10:34.690192Z",
     "shell.execute_reply.started": "2025-06-04T19:10:34.621179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = load_and_preprocess(DATASET_PATH, drop_aux=True, encode_binary=True, map_gender=True, one_hot_encoding=True, scale_numeric=True, to_numeric=True, encode_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of All Features with Target Variable\n",
    "\n",
    "Now that we have preprocessed the data, let's analyze the correlation of all features with the target variable `Churn`. This will help us understand which features are most relevant for predicting churn. \n",
    "\n",
    "Note: Since heatmap can be hard to read, especially after adding dummy variables, we use a bar plot to visualize the correlation coefficient for each feature with respect to churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:10:36.547985Z",
     "iopub.status.busy": "2025-06-04T19:10:36.546618Z",
     "iopub.status.idle": "2025-06-04T19:10:37.516301Z",
     "shell.execute_reply": "2025-06-04T19:10:37.514832Z",
     "shell.execute_reply.started": "2025-06-04T19:10:36.547887Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "churn_corr = df.corr(numeric_only=True)['Churn'].drop('Churn').sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.barplot(x=churn_corr.values, y=churn_corr.index, hue=churn_corr.index,\n",
    "            palette=\"coolwarm\", legend=False)\n",
    "plt.title(\"Feature Correlation with Churn\", fontsize=16)\n",
    "plt.xlabel(\"Correlation coefficient\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar plot shows how each feature is related to the target variable `Churn`. Features like `InternetService_Fiber optic` and `PaymentMethod_Electronic check` have the strongest positive correlations, meaning customers with these characteristics are more likely to leave. In contrast, features such as `Contract_Two year` and `tenure` show a negative correlation, suggesting these customers are more likely to stay.\n",
    "\n",
    "Many features have very low or near-zero correlation with churn. This doesnâ€™t mean they are unimportantâ€”it just means they may not have a simple linear relationship with the target. This plot adds to the earlier correlation analysis of numerical features and helps us understand which variables might be more useful for predicting churn.\n",
    "\n",
    "In summary, the correlation analysis reveals that customers with **fiber optic internet**, **electronic check payment**, and **higher monthly charges** are more likely to churn. In contrast, customers with **longer tenure**, **two-year contracts**, and **value-added services** like *Tech Support* and *Online Security* show lower churn rates. These patterns highlight which features are most predictive and will inform our feature selection and model design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "We will assess the performance of various classification models on the churn prediction tasks. The models we will evaluate include:\n",
    "1. **Logistic Regression**: A simple linear model for binary classification.\n",
    "2. **Random Forest**: An ensemble method that builds multiple decision trees.\n",
    "3. **Support Vector Machine**: A powerful model that finds the optimal hyperplane for classification.\n",
    "\n",
    "And we will also evaluate the **CatBoostClassifier**, which is outside of the course syllabus but is a powerful model for categorical data and can handle categorical features directly without preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "We will use the following evaluation metrics to assess model performance:\n",
    "1. Accuracy: The proportion of correct predictions out of total predictions.\n",
    "2. Precision: The proportion of true positive predictions out of all positive predictions.\n",
    "3. Recall: The proportion of true positive predictions out of all actual positive cases.\n",
    "4. F1_Score: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "5. roc_auc: The area under the ROC curve, which measures the model's ability to distinguish between classes.\n",
    "6. Confusion Matrix: A table that summarizes the performance of the classification model by showing true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:10:44.21739Z",
     "iopub.status.busy": "2025-06-04T19:10:44.216975Z",
     "iopub.status.idle": "2025-06-04T19:10:44.295982Z",
     "shell.execute_reply": "2025-06-04T19:10:44.294698Z",
     "shell.execute_reply.started": "2025-06-04T19:10:44.217359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = load_and_preprocess(\n",
    "    filepath=DATASET_PATH,\n",
    "    drop_aux=True,\n",
    "    encode_binary=True,\n",
    "    map_gender=True,\n",
    "    one_hot_encoding=True,\n",
    "    scale_numeric=True,\n",
    "    to_numeric=True,\n",
    ")\n",
    "\n",
    "# we will store the results of the models in a dictionary, later we will save them to a csv file\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance with SMOTE\n",
    "\n",
    "In our dataset, the target variable **Churn** is moderately imbalanced, with approximately **73% non-churn** and **27% churn** cases. This imbalance can lead to biased model performance, where classifiers favor the majority class (non-churn) and fail to correctly identify minority class instances (churned customers). Such a bias is especially problematic in churn prediction tasks, where identifying customers at risk of leaving is crucial for business actions.\n",
    "\n",
    "To address this, I employed **SMOTE** on the training data. SMOTE works by creating synthetic examples of the minority class based on the feature space similarities between existing minority samples. This method helps balance the dataset without simply duplicating existing instances.\n",
    "\n",
    "We applied SMOTE **only to the training set** to avoid data leakage and then retrained all models using the oversampled data. <span style=\"color: orange\">The evaluation was still performed on the original test set to ensure fair comparison.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pipeline for Preprocessing and Model Training\n",
    "\n",
    "To streamline the preprocessing and model training process, we will use `Pipeline` from `imblearn`. This allows us to combine preprocessing steps (like scaling and SMOTE) with model training in a single workflow. The pipeline will ensure that all preprocessing is applied consistently to both training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "For each model, we will perform hyperparameter tuning using `GridSearchCV` to find the best parameters that maximize the evaluation metrics. \n",
    "\n",
    "For scoring the models, Considering our target variable is imbalanced, we will use `roc_auc` as the scoring metric. This metric is suitable for imbalanced datasets as it evaluates the model's ability to distinguish between classes across all thresholds, rather than just at a single point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation\n",
    "\n",
    "To ensure robust model evaluation, we will use K-Fold cross-validation. This technique splits the dataset into K subsets (folds) and trains the model K times, each time using a different fold as the test set and the remaining folds as the training set. This helps us get a more reliable estimate of model performance by averaging results across multiple splits.\n",
    "We use 5-fold cross-validation for all models, which is a common choice that balances computational efficiency and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create or train-test split, using `train_test_split` from `sklearn.model_selection`. We will use 80% of the data for training and 20% for testing. We will also set a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:10:50.058179Z",
     "iopub.status.busy": "2025-06-04T19:10:50.057779Z",
     "iopub.status.idle": "2025-06-04T19:10:50.070488Z",
     "shell.execute_reply": "2025-06-04T19:10:50.069479Z",
     "shell.execute_reply.started": "2025-06-04T19:10:50.058152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y = df[TARGET_COLUMN]\n",
    "X = df.drop(columns=[TARGET_COLUMN])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with Logistic Regression, which is a simple yet effective model for binary classification tasks. We will use `LogisticRegression` from `sklearn.linear_model` and apply it within a pipeline that includes scaling and SMOTE.\n",
    "For hyperparameter tuning, we will use `GridSearchCV` to find the best parameters for the model. The parameters we will tune include:\n",
    "- `C`: Inverse of regularization strength (default is 1.0)\n",
    "- `penalty`: Type of regularization to apply (default is 'l2')\n",
    "- `max_iter`: Maximum number of iterations for convergence (default is 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:10:55.133294Z",
     "iopub.status.busy": "2025-06-04T19:10:55.132314Z",
     "iopub.status.idle": "2025-06-04T19:11:47.501102Z",
     "shell.execute_reply": "2025-06-04T19:11:47.499853Z",
     "shell.execute_reply.started": "2025-06-04T19:10:55.133262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters: {'clf__C': 0.01, 'clf__max_iter': 100, 'clf__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "# logistic regression pipeline with SMOTE and StandardScaler\n",
    "lr_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_STATE, solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "lr_param_grid = {\n",
    "    'clf__C': [0.01, 0.1, 1, 10, 100], # Regularization strength\n",
    "    'clf__penalty': ['l1', 'l2'], # Regularization type\n",
    "    'clf__max_iter': [100, 200, 300, 500, 1000] # Maximum number of iterations for convergence\n",
    "}\n",
    "\n",
    "lr_grid_search = GridSearchCV(\n",
    "    lr_pipeline,\n",
    "    lr_param_grid,\n",
    "    cv=K_FOLDS,\n",
    "    scoring=SCORING,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", lr_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we will evaluate its performance. We store the evaluation metrics in a dictionary for easy comparison later. Also, we will store some additional information about the model, such as the best parameters, confusion matrix, `fpr`, `tpr`, and `roc_auc` which will be used for plotting the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:18:00.468643Z",
     "iopub.status.busy": "2025-06-04T19:18:00.468269Z",
     "iopub.status.idle": "2025-06-04T19:18:00.517912Z",
     "shell.execute_reply": "2025-06-04T19:18:00.516783Z",
     "shell.execute_reply.started": "2025-06-04T19:18:00.468614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.71      0.80      1033\n",
      "           1       0.50      0.78      0.61       374\n",
      "\n",
      "    accuracy                           0.73      1407\n",
      "   macro avg       0.70      0.75      0.70      1407\n",
      "weighted avg       0.79      0.73      0.75      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of the model\n",
    "lr_y_pred = lr_grid_search.predict(X_test)\n",
    "\n",
    "lr_y_proba = lr_grid_search.predict_proba(X_test)[:, 1]\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_y_proba)\n",
    "lr_roc_auc = roc_auc_score(y_test, lr_y_proba)\n",
    "\n",
    "results[\"Logistic Regression\"] = {\n",
    "    \"values\": {\n",
    "        \"accuracy\": accuracy_score(y_test, lr_y_pred),\n",
    "        \"precision\": precision_score(y_test, lr_y_pred),\n",
    "        \"recall\": recall_score(y_test, lr_y_pred),\n",
    "        \"f1\": f1_score(y_test, lr_y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, lr_grid_search.predict_proba(X_test)[:, 1]),\n",
    "    },\n",
    "    \"roc_curve\": {\n",
    "        \"fpr\": lr_fpr,\n",
    "        \"tpr\": lr_tpr,\n",
    "        \"roc_auc\": lr_roc_auc,\n",
    "    },\n",
    "    \"confusion_matrix\": confusion_matrix(y_test, lr_y_pred),\n",
    "    \"model\": lr_grid_search.best_estimator_,\n",
    "    \"params\": lr_grid_search.best_params_,\n",
    "}\n",
    "print(classification_report(y_test, lr_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will evaluate the Support Vector Machine (SVM) classifier using `SVC` from `sklearn.svm`. We will also use a pipeline that includes scaling and SMOTE.\n",
    "\n",
    "We use `GridSearchCV` to tune the hyperparameters of the SVM model. The parameters we will tune include:\n",
    "- `C`: Regularization parameter (default is 1.0)\n",
    "- `kernel`: Type of kernel to use (default is 'rbf')\n",
    "- `gamma`: Kernel coefficient (default is 'scale')\n",
    "\n",
    "<span style=\"color: orange\">Note:</span> We don't use `poly` kernel for SVM, it causes the model to take too long to train, even with a small dataset like this one. We tested it (it took more than 20 minutes to train) and it didn't improve the performance and the best parameters were the same as the `linear` kernel. So we will stick with the `rbf` and `linear` kernels for our hyperparameter tuning.\n",
    "\n",
    "Here's the best parameters using `GridSearchCV` including `poly` kernel with `degree=[2,3,4]`\n",
    "\n",
    "\n",
    "```python\n",
    "svm_params = {\n",
    "    'clf__C': [0.1, 1, 10, 100],\n",
    "    'clf__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'clf__degree': [2, 3, 4],\n",
    "    'clf__gamma': ['scale', 'auto']\n",
    "}\n",
    "## Output\n",
    "{'clf__C': 0.1, 'clf__degree': 2, 'clf__gamma': 'scale', 'clf__kernel': 'linear'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:18:00.519298Z",
     "iopub.status.busy": "2025-06-04T19:18:00.51904Z",
     "iopub.status.idle": "2025-06-04T19:24:11.377663Z",
     "shell.execute_reply": "2025-06-04T19:24:11.376516Z",
     "shell.execute_reply.started": "2025-06-04T19:18:00.519272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters: {'clf__C': 0.1, 'clf__gamma': 'scale', 'clf__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "svm_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "svm_param_grid = {\n",
    "    'clf__C': [0.1, 1, 10], # Regularization parameter\n",
    "    'clf__kernel': ['linear', 'rbf'], # Kernel type\n",
    "    'clf__gamma': ['scale', 'auto'] # Kernel coefficient for 'rbf' kernel\n",
    "}\n",
    "svm_grid_search = GridSearchCV(\n",
    "    svm_pipeline,\n",
    "    svm_param_grid,\n",
    "    cv=K_FOLDS,\n",
    "    scoring=SCORING,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", svm_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:24:11.37935Z",
     "iopub.status.busy": "2025-06-04T19:24:11.3789Z",
     "iopub.status.idle": "2025-06-04T19:24:12.038889Z",
     "shell.execute_reply": "2025-06-04T19:24:12.037527Z",
     "shell.execute_reply.started": "2025-06-04T19:24:11.379317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.81      1033\n",
      "           1       0.52      0.75      0.62       374\n",
      "\n",
      "    accuracy                           0.75      1407\n",
      "   macro avg       0.71      0.75      0.71      1407\n",
      "weighted avg       0.79      0.75      0.76      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_y_pred = svm_grid_search.predict(X_test)\n",
    "\n",
    "svm_y_proba = svm_grid_search.predict_proba(X_test)[:, 1]\n",
    "svm_fpr, svm_tpr, _ = roc_curve(y_test, svm_y_proba)\n",
    "svm_roc_auc = roc_auc_score(y_test, svm_y_proba)\n",
    "\n",
    "results[\"SVM\"] = {\n",
    "    \"values\": {\n",
    "        \"accuracy\": accuracy_score(y_test, svm_y_pred),\n",
    "        \"precision\": precision_score(y_test, svm_y_pred),\n",
    "        \"recall\": recall_score(y_test, svm_y_pred),\n",
    "        \"f1\": f1_score(y_test, svm_y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, svm_grid_search.predict_proba(X_test)[:, 1]),\n",
    "    },\n",
    "    \"roc_curve\": {\n",
    "        \"fpr\": svm_fpr,\n",
    "        \"tpr\": svm_tpr,\n",
    "        \"roc_auc\": svm_roc_auc,\n",
    "    },\n",
    "    \"confusion_matrix\": confusion_matrix(y_test, svm_y_pred),\n",
    "    \"model\": svm_grid_search.best_estimator_,\n",
    "    \"params\": svm_grid_search.best_params_,\n",
    "}\n",
    "print(classification_report(y_test, svm_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "Next, we will evaluate the Random Forest classifier using `RandomForestClassifier` from `sklearn.ensemble`. We will also use a pipeline that includes scaling and SMOTE.\n",
    "We will use `GridSearchCV` to tune the hyperparameters of the Random Forest model. The parameters we will tune include:\n",
    "- `n_estimators`: Number of trees in the forest (default is 100)\n",
    "- `max_depth`: Maximum depth of the tree (default is None)\n",
    "- `min_samples_split`: Minimum number of samples required to split an internal node (default is 2)\n",
    "- `min_samples_leaf`: Minimum number of samples required to be at a leaf node (default is 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:24:12.040217Z",
     "iopub.status.busy": "2025-06-04T19:24:12.039954Z",
     "iopub.status.idle": "2025-06-04T19:27:27.367607Z",
     "shell.execute_reply": "2025-06-04T19:27:27.366494Z",
     "shell.execute_reply.started": "2025-06-04T19:24:12.040195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'clf__C': 0.01, 'clf__max_iter': 100, 'clf__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "rf_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "rf_param_grid = {\n",
    "    'clf__n_estimators': [50, 100, 200], # Number of trees in the forest\n",
    "    'clf__max_depth': [None, 10, 20, 30], # Maximum depth of the tree\n",
    "    'clf__min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node\n",
    "    'clf__min_samples_leaf': [1, 2, 4] # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    rf_param_grid,\n",
    "    cv=K_FOLDS,\n",
    "    scoring=SCORING,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", lr_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:27:27.371307Z",
     "iopub.status.busy": "2025-06-04T19:27:27.37101Z",
     "iopub.status.idle": "2025-06-04T19:27:27.560893Z",
     "shell.execute_reply": "2025-06-04T19:27:27.559735Z",
     "shell.execute_reply.started": "2025-06-04T19:27:27.371286Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.75      0.81      1033\n",
      "           1       0.51      0.72      0.60       374\n",
      "\n",
      "    accuracy                           0.74      1407\n",
      "   macro avg       0.70      0.74      0.70      1407\n",
      "weighted avg       0.78      0.74      0.75      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of Random Forest Classifier\n",
    "rf_y_pred = rf_grid_search.predict(X_test)\n",
    "\n",
    "rf_y_proba = rf_grid_search.predict_proba(X_test)[:, 1]\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_y_proba)\n",
    "rf_roc_auc = roc_auc_score(y_test, rf_y_proba)\n",
    "\n",
    "results[\"Random Forest\"] = {\n",
    "    \"values\": {\n",
    "        \"accuracy\": accuracy_score(y_test, rf_y_pred),\n",
    "        \"precision\": precision_score(y_test, rf_y_pred),\n",
    "        \"recall\": recall_score(y_test, rf_y_pred),\n",
    "        \"f1\": f1_score(y_test, rf_y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, rf_grid_search.predict_proba(X_test)[:, 1]),\n",
    "    },\n",
    "    \"roc_curve\": {\n",
    "        \"fpr\": rf_fpr,\n",
    "        \"tpr\": rf_tpr,\n",
    "        \"roc_auc\": rf_roc_auc,\n",
    "    },\n",
    "    \"confusion_matrix\": confusion_matrix(y_test, rf_y_pred),\n",
    "    \"model\": rf_grid_search.best_estimator_,\n",
    "    \"params\": rf_grid_search.best_params_,\n",
    "}\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Feature Importance\n",
    "\n",
    "A cool feature of Random Forest is its ability to provide feature importance scores. These scores indicate how much each feature contributes to the model's predictions. We can visualize the feature importances using a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:27:27.562104Z",
     "iopub.status.busy": "2025-06-04T19:27:27.561796Z",
     "iopub.status.idle": "2025-06-04T19:27:27.95764Z",
     "shell.execute_reply": "2025-06-04T19:27:27.956568Z",
     "shell.execute_reply.started": "2025-06-04T19:27:27.562081Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Feature importance for Random Forest\n",
    "importances = rf_grid_search.best_estimator_.named_steps['clf'].feature_importances_\n",
    "feature_names = X.columns\n",
    "indices = importances.argsort()[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances - Random Forest\")\n",
    "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), feature_names[indices], rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows `tenure`, `TotalCharges`, and `MonthlyCharges` as the top three most important features for predicting churn. This aligns with our earlier analysis, confirming that these numerical features are indeed significant predictors. `Contract_Two year` also has a high importance score, which is also aligns with our earlier findings that customers with longer contracts are less likely to churn. Other features like `PaymentMethod_Electronic check`, and `InternetService_Fiber optic` also contribute to the model's predictions, but to a lesser extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost Classifier\n",
    "Finally, we will evaluate the CatBoost classifier using `CatBoostClassifier` from the `catboost` library. CatBoost is a gradient boosting algorithm that can handle categorical features directly without the need for one-hot encoding or label encoding.\n",
    "For CatBoost, we don't need to use pipelines or scaling, as it can handle categorical features natively.\n",
    "\n",
    "We will have two steps of hyperparameter tuning:\n",
    "1. Using `cv` to find find the best estimate for iterations. Iterations is the number of boosting iterations, which is a crucial hyperparameter for CatBoost.\n",
    "2. Using `GridSearchCV` to tune the hyperparameters of the CatBoost model. The parameters we will tune include:\n",
    "- `depth`: Depth of the tree (default is 6)\n",
    "- `learning_rate`: Learning rate for boosting (default is 0.03)\n",
    "- `l2_leaf_reg`: L2 regularization coefficient (default is 3)\n",
    "- `early_stopping_rounds`: Number of rounds to stop training if no improvement (default is 50)\n",
    "\n",
    "For CatBoost, we also need to specify the categorical features in the model. We will use the `categorical_features` parameter to specify the categorical features in the dataset.\n",
    "\n",
    "#### Train and Test Pool\n",
    "We will create a `Pool` for training and testing data, which is a special data structure used by CatBoost to handle categorical features efficiently. The `Pool` will contain the training and testing data along with the target variable.\n",
    "\n",
    "<span style=\"color: orange\">Note:</span> Since CatBoost can handle categorical features directly, we don't need to preprocess the data like we did for other models. We will still use `load_and_preprocess` function to load the data, but we will not apply one-hot encoding or scaling for CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using class-weight in CatBoost\n",
    "\n",
    "To handle class imbalance in CatBoost, we can use the `class_weights` parameter. This parameter allows us to assign different weights to each class, which helps the model pay more attention to the minority class (churned customers). We will set the weight for the churned class to be higher than the non-churned class.\n",
    "For this example, I calculated the class weight manually based on the distribution of the target variable. The weight for the churned class is set to `[0.68, 1.88]`, which means the churned class will be given more importance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:27:27.959249Z",
     "iopub.status.busy": "2025-06-04T19:27:27.958989Z",
     "iopub.status.idle": "2025-06-04T19:27:28.022163Z",
     "shell.execute_reply": "2025-06-04T19:27:28.021178Z",
     "shell.execute_reply.started": "2025-06-04T19:27:27.959226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# we don't need one-hot encoding for CatBoost, it will handle categorical features automatically\n",
    "cb_df = load_and_preprocess(\n",
    "    filepath=DATASET_PATH,\n",
    "    drop_aux=True,\n",
    "    encode_binary=False,\n",
    "    map_gender=False,\n",
    "    one_hot_encoding=False,\n",
    "    scale_numeric=False,\n",
    "    to_numeric=True,\n",
    "    encode_target=True,\n",
    ")\n",
    "\n",
    "categorical_features = [\n",
    "    \"gender\",\n",
    "    \"Partner\",\n",
    "    \"Dependents\",\n",
    "    \"PhoneService\",\n",
    "    \"MultipleLines\",\n",
    "    \"InternetService\",\n",
    "    \"OnlineSecurity\",\n",
    "    \"OnlineBackup\",\n",
    "    \"DeviceProtection\",\n",
    "    \"TechSupport\",\n",
    "    \"StreamingTV\",\n",
    "    \"StreamingMovies\",\n",
    "    \"Contract\",\n",
    "    \"PaperlessBilling\",\n",
    "    \"PaymentMethod\",\n",
    "]\n",
    "\n",
    "cb_X = cb_df.drop(columns=[TARGET_COLUMN])\n",
    "cb_y = cb_df[TARGET_COLUMN]\n",
    "\n",
    "cb_X_train, cb_X_test, cb_y_train, cb_y_test = train_test_split(cb_X, cb_y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:27:28.023449Z",
     "iopub.status.busy": "2025-06-04T19:27:28.023165Z",
     "iopub.status.idle": "2025-06-04T19:27:28.146165Z",
     "shell.execute_reply": "2025-06-04T19:27:28.145052Z",
     "shell.execute_reply.started": "2025-06-04T19:27:28.023424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_pool = Pool(cb_X_train, cb_y_train, cat_features=categorical_features)\n",
    "test_pool = Pool(cb_X_test, cb_y_test, cat_features=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:27:28.148348Z",
     "iopub.status.busy": "2025-06-04T19:27:28.147908Z",
     "iopub.status.idle": "2025-06-04T19:27:49.677926Z",
     "shell.execute_reply": "2025-06-04T19:27:49.676903Z",
     "shell.execute_reply.started": "2025-06-04T19:27:28.148308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold [0/5]\n",
      "\n",
      "bestTest = 0.8493444654\n",
      "bestIteration = 164\n",
      "\n",
      "Training on fold [1/5]\n",
      "\n",
      "bestTest = 0.8431454323\n",
      "bestIteration = 162\n",
      "\n",
      "Training on fold [2/5]\n",
      "\n",
      "bestTest = 0.8523447812\n",
      "bestIteration = 239\n",
      "\n",
      "Training on fold [3/5]\n",
      "\n",
      "bestTest = 0.8456983326\n",
      "bestIteration = 320\n",
      "\n",
      "Training on fold [4/5]\n",
      "\n",
      "bestTest = 0.8599690656\n",
      "bestIteration = 232\n",
      "\n",
      "Best Iteration: 370\n"
     ]
    }
   ],
   "source": [
    "cb_cv_param_grid = {\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"eval_metric\": \"AUC\",\n",
    "    \"random_seed\": RANDOM_STATE,\n",
    "    \"verbose\": False,\n",
    "    \"iterations\": 1000,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "}\n",
    "cv_results = cv(\n",
    "    params=cb_cv_param_grid,\n",
    "    pool=train_pool,\n",
    "    fold_count=K_FOLDS,\n",
    "    partition_random_seed=RANDOM_STATE,\n",
    "    verbose=False,\n",
    "    # some additional package is required for plotting, https://catboost.ai/docs/en/installation/python-installation-additional-data-visualization-packages\n",
    "    plot=True, \n",
    ")\n",
    "\n",
    "# finding the best iteration\n",
    "best_iteration = cv_results['iterations'].max()\n",
    "print(f\"Best Iteration: {best_iteration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T19:27:49.679283Z",
     "iopub.status.busy": "2025-06-04T19:27:49.678915Z",
     "iopub.status.idle": "2025-06-04T20:38:36.772696Z",
     "shell.execute_reply": "2025-06-04T20:38:36.771216Z",
     "shell.execute_reply.started": "2025-06-04T19:27:49.679256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'depth': 4, 'early_stopping_rounds': 10, 'l2_leaf_reg': 3, 'learning_rate': 0.03}\n"
     ]
    }
   ],
   "source": [
    "cb_param_grid = {\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "    \"depth\": [4, 6, 8], # depth of the tree\n",
    "    \"l2_leaf_reg\": [1, 3, 5], # L2 regularization coefficient\n",
    "    \"early_stopping_rounds\": [10, 50, 100], # number of rounds to stop training if no improvement\n",
    "}\n",
    "cb_classifier = CatBoostClassifier(\n",
    "    iterations=best_iteration,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    class_weights=[0.68, 1.88],\n",
    "    random_seed=RANDOM_STATE,\n",
    "    verbose=False\n",
    ")\n",
    "cb_grid_search = GridSearchCV(\n",
    "    cb_classifier,\n",
    "    cb_param_grid,\n",
    "    cv=K_FOLDS,\n",
    "    scoring=SCORING,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "cb_grid_search.fit(cb_X_train, cb_y_train, cat_features=categorical_features)\n",
    "print(\"Best parameters:\", cb_grid_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T20:38:36.775121Z",
     "iopub.status.busy": "2025-06-04T20:38:36.774683Z",
     "iopub.status.idle": "2025-06-04T20:38:36.840069Z",
     "shell.execute_reply": "2025-06-04T20:38:36.839028Z",
     "shell.execute_reply.started": "2025-06-04T20:38:36.775085Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.70      0.79      1033\n",
      "           1       0.49      0.81      0.61       374\n",
      "\n",
      "    accuracy                           0.73      1407\n",
      "   macro avg       0.70      0.76      0.70      1407\n",
      "weighted avg       0.80      0.73      0.74      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation of CatBoost Classifier\n",
    "cb_y_pred = cb_grid_search.predict(cb_X_test)\n",
    "\n",
    "cb_y_proba = cb_grid_search.predict_proba(cb_X_test)[:, 1]\n",
    "cb_fpr, cb_tpr, _ = roc_curve(cb_y_test, cb_y_proba)\n",
    "cb_roc_auc = roc_auc_score(cb_y_test, cb_y_proba)\n",
    "\n",
    "results[\"CatBoost\"] = {\n",
    "    \"values\": {\n",
    "        \"accuracy\": accuracy_score(cb_y_test, cb_y_pred),\n",
    "        \"precision\": precision_score(cb_y_test, cb_y_pred),\n",
    "        \"recall\": recall_score(cb_y_test, cb_y_pred),\n",
    "        \"f1\": f1_score(cb_y_test, cb_y_pred),\n",
    "        \"roc_auc\": roc_auc_score(cb_y_test, cb_grid_search.predict_proba(cb_X_test)[:, 1]),\n",
    "    },\n",
    "    \"roc_curve\": {\n",
    "        \"fpr\": cb_fpr,\n",
    "        \"tpr\": cb_tpr,\n",
    "        \"roc_auc\": cb_roc_auc,\n",
    "    },\n",
    "    \"confusion_matrix\": confusion_matrix(cb_y_test, cb_y_pred),\n",
    "    \"model\": cb_grid_search.best_estimator_,\n",
    "    \"params\": cb_grid_search.best_params_,\n",
    "}\n",
    "print(classification_report(cb_y_test, cb_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "\n",
    "Now that we have trained all the models, we can compare their performance based on the evaluation metrics we defined earlier. We will create a summary table to display the results of each model, including accuracy, precision, recall, F1-score, ROC-AUC, and confusion matrix. We also plot the ROC curves for each model to visualize their performance. Additionally, we will plot the confusion matrices for each model to see how well they classify churned and non-churned customers.\n",
    "\n",
    "#### Plotting Model Performance\n",
    "First let's create a plot, showing the performance of each model based on the evaluation metrics. We will use a bar plot to visualize the accuracy, precision, recall, F1-score, and ROC-AUC for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T20:38:36.841365Z",
     "iopub.status.busy": "2025-06-04T20:38:36.841076Z",
     "iopub.status.idle": "2025-06-04T20:38:37.325332Z",
     "shell.execute_reply": "2025-06-04T20:38:37.324202Z",
     "shell.execute_reply.started": "2025-06-04T20:38:36.841343Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# comparision of model metrics\n",
    "dynamic_df = pd.DataFrame.from_dict(\n",
    "    {k: v[\"values\"] for k, v in results.items()},\n",
    "    orient='index'\n",
    ").reset_index().rename(columns={'index': 'Model'})\n",
    "dynamic_df = dynamic_df.melt(id_vars='Model', var_name='Metric', value_name='Value')\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=dynamic_df, x='Model', y='Value', hue='Metric')\n",
    "plt.title('Model Comparison Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing Results\n",
    "\n",
    "Now, let's take a look at the same data in a tabular format for better readability. We will create a DataFrame to summarize the evaluation metrics for each model. We will also store the result in a CSV file for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T20:38:37.326654Z",
     "iopub.status.busy": "2025-06-04T20:38:37.326388Z",
     "iopub.status.idle": "2025-06-04T20:38:37.359148Z",
     "shell.execute_reply": "2025-06-04T20:38:37.357937Z",
     "shell.execute_reply.started": "2025-06-04T20:38:37.326633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "results_df = pd.DataFrame.from_dict(\n",
    "    {model: res[\"values\"] for model, res in results.items()},\n",
    "    orient='index'\n",
    ")\n",
    "\n",
    "# save results to CSV\n",
    "results_df.to_csv(f\"{RESULT_PATH}/{RESULT_FINE_NAME}-{datetime.now().strftime('%Y%m%d_%H%M%S')}.{RESULT_FINE_EXT}\")\n",
    "\n",
    "# The \"RESULT_FINE_NAME\" always contains the latest results\n",
    "results_df.to_csv(f\"{RESULT_PATH}/{RESULT_FINE_NAME}.{RESULT_FINE_EXT}\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "Next, we will plot the confusion matrices for each model to visualize how well they classify churned and non-churned customers. The confusion matrix shows the number of true positives, true negatives, false positives, and false negatives for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T20:38:37.361529Z",
     "iopub.status.busy": "2025-06-04T20:38:37.361136Z",
     "iopub.status.idle": "2025-06-04T20:38:38.739636Z",
     "shell.execute_reply": "2025-06-04T20:38:38.738563Z",
     "shell.execute_reply.started": "2025-06-04T20:38:37.361497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# confusion matrices\n",
    "n_models = len(results)\n",
    "cols = 2\n",
    "rows = (n_models + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "axes = axes.flatten() if n_models > 1 else [axes]\n",
    "for i, (model_name, res) in enumerate(results.items()):\n",
    "    cm = res[\"confusion_matrix\"]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "    axes[i].set_title(f\"{model_name} Confusion Matrix\")\n",
    "    axes[i].set_xlabel(\"Predicted\")\n",
    "    axes[i].set_ylabel(\"Actual\")\n",
    "    axes[i].set_xticklabels([\"No Churn\", \"Churn\"])\n",
    "    axes[i].set_yticklabels([\"No Churn\", \"Churn\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve\n",
    "Finally, we will plot the ROC curves for each model to visualize their performance in terms of true positive rate (sensitivity) and false positive rate (1-specificity). The ROC curve shows how well the model distinguishes between churned and non-churned customers at different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T20:38:38.741669Z",
     "iopub.status.busy": "2025-06-04T20:38:38.741102Z",
     "iopub.status.idle": "2025-06-04T20:38:39.007021Z",
     "shell.execute_reply": "2025-06-04T20:38:39.006003Z",
     "shell.execute_reply.started": "2025-06-04T20:38:38.74164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for model_name, res in results.items():\n",
    "    fpr = res[\"roc_curve\"][\"fpr\"]\n",
    "    tpr = res[\"roc_curve\"][\"tpr\"]\n",
    "    auc_score = res[\"roc_curve\"][\"roc_auc\"]\n",
    "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {auc_score:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('ROC Curves for Different Models')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC Curve Analysis\n",
    "\n",
    "The **ROC curve** helps evaluate how well each model can distinguish between churners and non-churners. It plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** across different thresholds. The main summary metric is the **Area Under the Curve (AUC)**, the higher the AUC, the better the model is at classification.\n",
    "\n",
    "From the ROC curve and AUC scores:\n",
    "\n",
    "- **CatBoost** achieved the highest AUC of **0.84**, showing it is the best at separating the two classes among all models tested. This indicates that CatBoost has strong ranking performance, even though its accuracy is not the highest.\n",
    "- **Random Forest** follows closely with an AUC of **0.829**, also performing well in distinguishing churners from non-churners.\n",
    "- **Logistic Regression** and **SVM** both performed similarly with AUC scores around **0.825** and **0.823**, respectively. These models are still effective, but slightly weaker than the ensemble methods.\n",
    "\n",
    "Overall, **all four models performed better than random guessing (AUC = 0.5)**, and ensemble models like CatBoost and Random Forest provided more reliable class separation, making them strong candidates for this churn prediction task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Comparison\n",
    "\n",
    "To evaluate the models on the Telco Customer Churn dataset, we used five classification metrics: **Accuracy**, **Precision**, **Recall**, **F1-score**, and **ROC AUC**. These metrics help us understand the models' strengths and weaknesses, especially since the dataset is imbalanced, with about 26.6% of customers labeled as churners.\n",
    "\n",
    "| Model                | Accuracy | Precision | Recall | F1-score | ROC AUC |\n",
    "|---------------------|----------|-----------|--------|----------|---------|\n",
    "| Logistic Regression | 0.731    | 0.497     | 0.781  | 0.607    | 0.826   |\n",
    "| SVM                 | 0.749    | 0.519     | 0.754  | 0.615    | 0.823   |\n",
    "| Random Forest       | 0.741    | 0.509     | 0.725  | 0.598    | 0.829   |\n",
    "| CatBoost            | 0.729    | 0.494     | **0.810**  | 0.614    | **0.840**   |\n",
    "\n",
    "#### Accuracy\n",
    "The SVM achieved the highest accuracy at 0.749, which means it made the most correct predictions overall. However, because the dataset is imbalanced, accuracy alone is not a reliable measure. A model may achieve high accuracy simply by predicting the majority class more often.\n",
    "\n",
    "#### Precision and Recall\n",
    "- **Precision** reflects how many of the predicted churners were actually correct. SVM had the highest precision (0.519), showing it was more selective when predicting churn.\n",
    "- **Recall** shows how many actual churners were successfully identified. CatBoost performed best in this metric (0.810), meaning it identified the most churners, which is important for customer retention strategies.\n",
    "\n",
    "#### F1-score\n",
    "The F1-score is the harmonic mean of precision and recall. SVM and CatBoost scored similarly (0.615 and 0.614), showing that both have a balanced trade-off. SVM slightly favors precision, while CatBoost favors recall.\n",
    "\n",
    "#### ROC AUC\n",
    "The ROC AUC score shows the modelâ€™s ability to separate churners from non-churners across all thresholds. CatBoost achieved the highest score (0.840), indicating it is the best at ranking customers by their likelihood to churn.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Each model has its own strengths:\n",
    "\n",
    "- **CatBoost** is the most suitable when the goal is to capture as many churners as possible. It has the highest recall and AUC, making it ideal for reducing customer loss.\n",
    "- **SVM** is more balanced and has the highest precision, which could be useful when false positives are costly.\n",
    "- **Random Forest** shows consistent performance but does not lead in any specific metric.\n",
    "- **Logistic Regression** performs reasonably but is slightly weaker than the other models in most aspects.\n",
    "\n",
    "In conclusion, **CatBoost** is the best model for this churn prediction task due to its strong overall performance, especially in recall and AUC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering means creating new features or changing existing ones to help the model learn better. It can improve the modelâ€™s performance by showing patterns in the data that are not easy to see at first.\n",
    "\n",
    "In this part, we create an **interaction feature** using two numerical columns. An interaction feature helps the model understand how two values work together.\n",
    "\n",
    "### Create Interaction Feature\n",
    "\n",
    "We added a new feature, `tenure_MonthlyCharges`, which is the product of `tenure` and `MonthlyCharges`. This feature captures the relationship between how long a customer has been with the company and how much they pay monthly. It can help the model understand that customers who have been with the company longer and pay more might be less likely to churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T20:38:39.008749Z",
     "iopub.status.busy": "2025-06-04T20:38:39.008386Z",
     "iopub.status.idle": "2025-06-04T20:38:39.443861Z",
     "shell.execute_reply": "2025-06-04T20:38:39.443116Z",
     "shell.execute_reply.started": "2025-06-04T20:38:39.008722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "df = load_and_preprocess(\n",
    "    filepath=DATASET_PATH,\n",
    "    drop_aux=True,\n",
    "    encode_binary=True,\n",
    "    map_gender=True,\n",
    "    one_hot_encoding=True,\n",
    "    scale_numeric=True,\n",
    "    to_numeric=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Split features/target\n",
    "X = df.drop(columns=[TARGET_COLUMN])\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "# Model 1: Baseline Logistic Regression\n",
    "lr1 = LogisticRegression(random_state=42, max_iter=500)\n",
    "lr1.fit(X_train, y_train)\n",
    "y_pred1 = lr1.predict(X_test)\n",
    "y_proba1 = lr1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "results = {}\n",
    "results[\"Logistic Regression (no new feature)\"] = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred1),\n",
    "    \"precision\": precision_score(y_test, y_pred1),\n",
    "    \"recall\": recall_score(y_test, y_pred1),\n",
    "    \"f1\": f1_score(y_test, y_pred1),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_proba1),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T20:38:39.448315Z",
     "iopub.status.busy": "2025-06-04T20:38:39.447431Z",
     "iopub.status.idle": "2025-06-04T20:38:39.858071Z",
     "shell.execute_reply": "2025-06-04T20:38:39.857315Z",
     "shell.execute_reply.started": "2025-06-04T20:38:39.448278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Model 2: With Interaction Feature\n",
    "X_train2 = X_train.copy()\n",
    "X_test2 = X_test.copy()\n",
    "X_train2['tenure_MonthlyCharges'] = X_train2['tenure'] * X_train2['MonthlyCharges']\n",
    "X_test2['tenure_MonthlyCharges'] = X_test2['tenure'] * X_test2['MonthlyCharges']\n",
    "\n",
    "lr2 = LogisticRegression(random_state=42, max_iter=500)\n",
    "lr2.fit(X_train2, y_train)\n",
    "y_pred2 = lr2.predict(X_test2)\n",
    "y_proba2 = lr2.predict_proba(X_test2)[:, 1]\n",
    "\n",
    "results[\"Logistic Regression (with interaction feature)\"] = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred2),\n",
    "    \"precision\": precision_score(y_test, y_pred2),\n",
    "    \"recall\": recall_score(y_test, y_pred2),\n",
    "    \"f1\": f1_score(y_test, y_pred2),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_proba2),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T20:38:39.860253Z",
     "iopub.status.busy": "2025-06-04T20:38:39.859892Z",
     "iopub.status.idle": "2025-06-04T20:38:39.87738Z",
     "shell.execute_reply": "2025-06-04T20:38:39.876414Z",
     "shell.execute_reply.started": "2025-06-04T20:38:39.860227Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df.to_csv(f\"{RESULT_PATH}/{RESULT_FINE_NAME}-interaction-feature.{RESULT_FINE_EXT}\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Analysis\n",
    "\n",
    "Feature engineering is the process of creating new variables or modifying existing ones to help machine learning models learn better from the data. In this case, we introduced an **interaction feature** by multiplying two numerical features: `tenure` and `MonthlyCharges`. The new feature, named `tenure_MonthlyCharges`, is designed to capture the relationship between how long a customer has been with the company and how much they pay each month.\n",
    "\n",
    "This interaction can provide extra information to the model. For example, customers who have both high tenure and high monthly charges might behave differently in terms of churn compared to those with only one of these factors.\n",
    "\n",
    "To evaluate the effect of this feature, we used **Logistic Regression** as a **baseline model**. Logistic Regression was chosen because it is simple, interpretable, and sensitive to linear relationships between features and the target variable. This makes it a good starting point to observe the impact of new feature additions.\n",
    "\n",
    "We then compared the model performance with and without the new interaction feature:\n",
    "\n",
    "| Model                                | Accuracy | Precision | Recall | F1-score | ROC AUC |\n",
    "|-------------------------------------|----------|-----------|--------|----------|---------|\n",
    "| Logistic Regression (original)      | 0.787    | 0.619     | 0.513  | 0.561    | 0.832   |\n",
    "| Logistic Regression (with feature)  | **0.789** | **0.624** | **0.519** | **0.566** | **0.832** |\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "- After adding the interaction feature, all performance metrics improved slightly, including accuracy, precision, recall, and F1-score.\n",
    "- The ROC AUC score remained nearly the same (0.832), suggesting that the modelâ€™s ability to rank churners versus non-churners did not change significantly. However, the improvements in other metrics show better predictive performance.\n",
    "- These results indicate that the new feature contributes positively to the model's ability to separate the classes.\n",
    "\n",
    "Although the improvements were modest, they were consistent across all key metrics. This shows that even a simple interaction feature can help the model perform better. The experiment also demonstrates how Logistic Regression, due to its simplicity, can serve as an effective baseline for evaluating feature engineering efforts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project explored customer churn prediction using the Telco Customer Churn dataset through a complete machine learning pipeline: from data cleaning and exploration to model evaluation and feature engineering. The dataset was moderately imbalanced, requiring techniques like SMOTE to address bias during training.\n",
    "\n",
    "Exploratory Data Analysis (EDA) revealed several important patterns. Features such as `tenure`, `MonthlyCharges`, `Contract`, and use of services like `TechSupport` or `OnlineSecurity` showed strong associations with customer churn. We also identified multicollinearity between `TotalCharges` and other numerical features, which informed our model selection and feature usage strategies.\n",
    "\n",
    "Multiple classification models were developed and compared using consistent preprocessing pipelines and evaluation metrics. Among them:\n",
    "\n",
    "- **CatBoost** achieved the highest AUC (0.840) and recall (0.81), making it the most effective model for identifying churned customers.\n",
    "- **SVM** offered the best balance between precision and recall and had the highest accuracy (0.749).\n",
    "- **Random Forest** provided consistent performance across all metrics, and its feature importance analysis confirmed the value of previously identified predictors.\n",
    "- **Logistic Regression**, used as a baseline, also performed reasonably well and was extended in a feature engineering experiment.\n",
    "\n",
    "We introduced a new interaction feature (`tenure * MonthlyCharges`) and evaluated its effect using Logistic Regression. The addition led to small but consistent improvements in accuracy, precision, recall, and F1-score, confirming the benefit of feature engineering.\n",
    "\n",
    "In conclusion, CatBoost is the most suitable model for churn prediction in this context, especially when recall and ranking performance are critical. The project also highlights the importance of proper preprocessing, careful metric selection for imbalanced data, and the value of domain-driven feature engineering. Future work could explore additional interaction features, model ensembles, or deployment strategies for real-time churn risk scoring.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 13996,
     "isSourceIdPinned": false,
     "sourceId": 18858,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "csca-5622-supervised-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
